{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T21:49:08.796005Z",
     "start_time": "2020-01-10T21:48:57.441386Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from six.moves import xrange\n",
    "from pprint import pprint\n",
    "from math import sqrt\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization as BN\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "rn.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T05:38:16.080039Z",
     "start_time": "2020-01-13T05:38:16.073166Z"
    }
   },
   "outputs": [],
   "source": [
    "inputdf = pd.read_csv(\"./New Data/inputdf.csv\")\n",
    "outputdf = pd.read_csv(\"./New Data/outputdf.csv\")\n",
    "print (\"Data Read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T23:00:48.936547Z",
     "start_time": "2020-01-11T23:00:48.921810Z"
    }
   },
   "outputs": [],
   "source": [
    "def calMetric(yTrue,yPred,i):\n",
    "    '''\n",
    "    Mean Absolute Percentage Error.\n",
    "    '''\n",
    "    mape = 0\n",
    "    for j in range(len(yTrue)):\n",
    "        mape += (abs(yTrue[j]-yPred[j])/yTrue[j])\n",
    "    mape = (mape*1.0/len(yTrue))*100    \n",
    "\n",
    "    print \"\\n\"\n",
    "    print outputdf.columns[i]\n",
    "    print \"R2 : \",r2_score(yTrue,yPred)\n",
    "    print \"MAE : \",mean_absolute_error(yTrue,yPred)\n",
    "    print \"RMSE : \",sqrt(mean_squared_error(yTrue,yPred))\n",
    "    print \"MAPE : \",mape\n",
    "    print (\"----------------------------------------------------------\")\n",
    "    return  [r2_score(yTrue,yPred), mean_absolute_error(yTrue,yPred), sqrt(mean_squared_error(yTrue,yPred)),mape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T22:57:26.054831Z",
     "start_time": "2020-01-11T22:57:26.047009Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get the Train and the Test indices.\n",
    "'''\n",
    "def train_test_indices(df):\n",
    "    k = np.arange(df.shape[0])\n",
    "    np.random.shuffle(k)\n",
    "    t = (int)(0.8*len(k))\n",
    "    train,test = k[0:t],k[t:]\n",
    "    return train,test,k,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T22:57:28.103689Z",
     "start_time": "2020-01-11T22:57:28.085318Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to perform Normalization.\n",
    "'''\n",
    "def normalize_df(totaldf,df,odf):\n",
    "    \n",
    "    #Don't normalize the last 6  features.\n",
    "    for i in range(2,len(df.columns)-6):\n",
    "        mx = max(totaldf.iloc[:,i].values)\n",
    "        #mx = 2**(math.ceil(math.log(max(inputdf.iloc[:,i].values), 2)))\n",
    "        mn = min(totaldf.iloc[:,i].values)\n",
    "        \n",
    "        df.iloc[:,i] = (df.iloc[:,i].values - mn)*1.0 / float(mx-mn)\n",
    "        odf.iloc[:,i] = (odf.iloc[:,i].values - mn)*1.0 / float(mx-mn)\n",
    "        \n",
    "        #Now, if any Nans/ Infs are present, replace them with '0'.\n",
    "        df = df.fillna(0)\n",
    "        df = df.replace([np.inf],[0])\n",
    "        \n",
    "        odf = odf.fillna(0)\n",
    "        odf = odf.replace([np.inf],[0])\n",
    "        \n",
    "    return df,odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T22:57:32.443243Z",
     "start_time": "2020-01-11T22:57:32.407910Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Custom Loss Function\n",
    "'''\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    \n",
    "    error = y_true - y_pred\n",
    "    cond  = K.abs(error) < clip_delta\n",
    "    \n",
    "    error_calc = K.abs((y_pred-y_true))*1.0/y_true\n",
    "    \n",
    "    val = tf.placeholder(tf.float32)\n",
    "    #val = tf.add( tf.cast(tf.logical_and( tf.greater_equal(error_calc,tf.constant(15.0)) , tf.less_equal(error_calc,tf.constant(25.0))),tf.float32) , 1)\n",
    "    \n",
    "    #Check for the interval : 15-25%.\n",
    "    val = tf.where( tf.logical_and( tf.greater(error_calc,tf.constant([[15.0]],dtype=tf.float32)) , tf.less_equal(error_calc,tf.constant([[25.0]],dtype=tf.float32))) , tf.constant([[2.0]],dtype=tf.float32), tf.constant([[1.0]],dtype=tf.float32))\n",
    "    \n",
    "    #Check for the interval : 25-35%.\n",
    "    val = tf.where( tf.logical_and( tf.greater(error_calc,tf.constant([[25.0]],dtype=tf.float32)) , tf.less_equal(error_calc,tf.constant([[35.0]],dtype=tf.float32))) , tf.constant([[2.25]],dtype=tf.float32), tf.constant([[1.0]],dtype=tf.float32))\n",
    "    \n",
    "    squared_loss = 0.5 * K.square(error) * val\n",
    "    linear_loss  = clip_delta * (K.abs(error) - 0.5 * clip_delta) * val\n",
    "    \n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "def nn_model_init(xTrain):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(300, input_dim = xTrain.shape[1], activation='relu',activity_regularizer=regularizers.l1(0.01)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(300, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss=tf.losses.huber_loss,optimizer='Adam',metrics=['mae'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T22:57:33.814484Z",
     "start_time": "2020-01-11T22:57:33.802584Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn_model(xTrain,yTrain,xTest,yTest,all_days,model,size):\n",
    "        \n",
    "        '''\n",
    "        Only train the model, when in a particular week, there is atleast one point.\n",
    "        '''\n",
    "        print(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)\n",
    "        \n",
    "        if (size>0):\n",
    "            print \"Training the model.\"\n",
    "            callbacks1 = [callbacks.EarlyStopping(monitor='mean_absolute_error',patience=20, verbose=0)]\n",
    "            model.fit(xTrain,yTrain,epochs=500,batch_size=7,verbose=2,callbacks=callbacks1)\n",
    "        \n",
    "        yPred = model.predict(xTest,batch_size=7)\n",
    "        \n",
    "        return yPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised 0 to 1 absolute plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T11:09:12.601295Z",
     "start_time": "2018-04-09T11:04:34.360128Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "'''\n",
    "INCORPORATION OF MODEL1-MODEL2.\n",
    "\n",
    "NN Model\n",
    "Processing data, for the model. Normalized!!!\n",
    "Features :\n",
    "(1) 4 columns pertaining to the language wise occupancies.l\n",
    "(2) Moving Average of food per occ of food items. [2,3,7,10 days of MA]\n",
    "Total Features : 8\n",
    "\n",
    "With Dropout.\n",
    "'''\n",
    "\n",
    "master_ypred = {}\n",
    "master_ytest = {}\n",
    "master_mape = {}\n",
    "history2 = {}\n",
    "indices = {}\n",
    "master_dict = {}\n",
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "outputdf = outputdf[outputdf.columns[0:14]]\n",
    "'''\n",
    "Get the indices for the weekdays and weekends in 2017.\n",
    "'''\n",
    "\n",
    "for i in range(0, 14):\n",
    "    totaldf = pd.DataFrame()\n",
    "    df = pd.DataFrame()\n",
    "    odf = pd.DataFrame()\n",
    "    tdf = pd.DataFrame()\n",
    "    todf = pd.DataFrame()\n",
    "    dates = []\n",
    "    mape = []\n",
    "    df,odf,tdf,todf,o_indices = init_dfs(df,odf,tdf,todf)\n",
    "    \n",
    "    start = tdf.iloc[0][0]\n",
    "    end = tdf.iloc[tdf.shape[0]-1][0]\n",
    "    \n",
    "    print df.shape,odf.shape,tdf.shape,todf.shape\n",
    "    o_indices = list(tdf.index)\n",
    "    totaldf = df.copy()\n",
    "    \n",
    "    loopcounter = 0\n",
    "    model_init_counter = 0\n",
    "    \n",
    "    master_dict[i] = {}\n",
    "    master_ypred[i] = []\n",
    "    master_ytest[i] = []\n",
    "    master_mape[i] = []\n",
    "    \n",
    "    history2 = {}\n",
    "    \n",
    "    while(1):\n",
    "        errcategory = []\n",
    "        print \"\\n\\n\"\n",
    "        print \"--------------------------------------------------------------\"\n",
    "        indices[loopcounter] = []\n",
    "        history2[tdf['Date'][0]] = []\n",
    "        local = []\n",
    "        counter = 0\n",
    "        '''\n",
    "        Initially, df and odf are defined.\n",
    "\n",
    "        Normalization should occur, each and every time, a new batch comes in.\n",
    "\n",
    "        Make some changes here, where df refers to the current training dataframe.\n",
    "        For each food, initially 'df' will be containing the first month of training.\n",
    "        '''\n",
    "        print \"Shape of Train df, Test df : \",df.shape,tdf.shape\n",
    "        print \"Train between : \",df.iloc[0][0],\" - \",df.iloc[-1][0]\n",
    "        start_date += [df.iloc[0][0]]\n",
    "        end_date += [df.iloc[-1][0]]\n",
    "        print \"Test between  : \",tdf.iloc[0][0],\" - \",tdf.iloc[-1][0]\n",
    "        tdf = tdf.fillna(0)\n",
    "        tdf = tdf.replace([np.inf],[0])\n",
    "        \n",
    "        df = df.fillna(0)\n",
    "        df = df.replace([np.inf],[0])\n",
    "        # Normalize both the input and the output at the same time.\n",
    "        df,tdf = normalize_df(totaldf,df, tdf)\n",
    "        \n",
    "        if(model_init_counter==0):\n",
    "            df2 = df\n",
    "            odf2 = odf\n",
    "        else:\n",
    "            #Split df into df and df2 based on holiday / not holiday.\n",
    "            req_indices = list(df[df['Holiday/Not Holiday of tomorrow']==0].index)\n",
    "            df2 = df[df['Holiday/Not Holiday of tomorrow']==0]\n",
    "            odf2 = odf.iloc[req_indices,:]\n",
    "            df2.reset_index(drop=True,inplace=True)\n",
    "            odf2.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        #Split tdf, into tdf and tdf2 based on holiday/not holiday.\n",
    "        req_indices = list(tdf[tdf['Holiday/Not Holiday of tomorrow']==0].index)\n",
    "        tdf2 = tdf[tdf['Holiday/Not Holiday of tomorrow']==0]\n",
    "        todf2 = todf.iloc[req_indices,:]\n",
    "        tdf2.reset_index(drop=True,inplace=True)\n",
    "        todf2.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        req_indices = list(tdf[tdf['Holiday/Not Holiday of tomorrow']==1].index)\n",
    "        tdf = tdf[tdf['Holiday/Not Holiday of tomorrow']==1]\n",
    "        todf = todf.iloc[req_indices,:]\n",
    "        tdf.reset_index(drop=True,inplace=True)\n",
    "        todf.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        #First get the holiday dates, then the non-holiday dates, in a particular week.\n",
    "        dates += list(tdf['Date'])\n",
    "        dates += list(tdf2['Date'])\n",
    "        \n",
    "        test_indices = range(tdf.shape[0])\n",
    "        xTrain = ((df.values[:,[485,486,487] + range(2,6)]))\n",
    "        xTrain2 = ((df2.values[:,[485,486,487] + range(2,6)]))\n",
    "        xTest = ((tdf.values[:, [485,486,487] + range(2,6)]))\n",
    "        xTest2 = ((tdf2.values[:, [485,486,487] + range(2,6)]))\n",
    "        \n",
    "        lst = [6, 20, 34, 48, 426]\n",
    "\n",
    "        for j in lst:\n",
    "            xTrain = np.hstack((xTrain, df.values[:,i+j].reshape(-1, 1)))\n",
    "            xTrain2 = np.hstack((xTrain2, df2.values[:,i+j].reshape(-1, 1)))\n",
    "            xTest = np.hstack((xTest, tdf.values[:,i+j].reshape(-1, 1),))\n",
    "            xTest2 = np.hstack((xTest2, tdf2.values[:,i+j].reshape(-1, 1),))\n",
    "        \n",
    "        yTrain = odf.values[:,i]\n",
    "        yTrain2 = odf2.values[:,i]\n",
    "        yTest = todf.values[:,i]\n",
    "        yTest2 = todf2.values[:,i]\n",
    "\n",
    "        print(\"\\n\")\n",
    "        if (model_init_counter==0):\n",
    "            model = nn_model_init(xTrain)\n",
    "            model2 = nn_model_init(xTrain2)\n",
    "        '''\n",
    "        xTest , yTest will be for 'model'. (only predicting for holidays.)\n",
    "        xTest2 , yTest2 will be for 'model2'. (only predicting for non-holidays.)\n",
    "        '''\n",
    "        print \"------------------------------------------------------\"\n",
    "        print \"Shapes : \"\n",
    "        print \"xTrain , yTrain , xTest , yTest : \",xTrain.shape,yTrain.shape,xTest.shape,yTest.shape\n",
    "        print \"xTrain2 , yTrain2 , xTest2 , yTest2 : \",xTrain2.shape,yTrain2.shape,xTest2.shape,yTest2.shape\n",
    "        print \"------------------------------------------------------\"\n",
    "        # 'model' trains on everything present.\n",
    "        print\"-------------------------------------------------------\"\n",
    "        print \"Training Model 1\"\n",
    "        yPred = nn_model(xTrain, yTrain, xTest, yTest, test_indices, model,df.shape[0])\n",
    "        print \"-------------------------------------------------------\"\n",
    "        print \"Training Model 2\"\n",
    "        # 'model2' selectively trains on weekdays, and is used to predict for weekdays.\n",
    "        yPred2 = nn_model(xTrain2, yTrain2, xTest2, yTest2, test_indices, model2,df2.shape[0])\n",
    "        yPred = np.concatenate((yPred,yPred2))\n",
    "        \n",
    "        for looper in range(yPred.shape[0]):\n",
    "            master_ypred[i] += list(yPred[looper])\n",
    "        \n",
    "        #Combine both yTest and yTest2 into yTest, in the format, HOLIDAY followed by NOT HOLIDAY.\n",
    "        yTest = np.concatenate((yTest,yTest2))\n",
    "        master_ytest[i] += list(yTest)\n",
    "        \n",
    "        templist = mape_calc(yTest,yPred)\n",
    "        master_mape[i] += templist\n",
    "        \n",
    "        local.append(calMetric(yTest, yPred, i))\n",
    "        \n",
    "        errcategory = calculate_err_category(templist)\n",
    "        # Sequential test, train split.\n",
    "        \n",
    "        #Recombine tdf and tdf2 into tdf, for computation of the next batch.\n",
    "        tdf = pd.concat([tdf,tdf2],axis=0)\n",
    "        tdf.reset_index(drop=True,inplace=True)\n",
    "        print \"Null(s) Check : \",np.sum(tdf.isnull().sum(axis=1))\n",
    "        \n",
    "        # New train set is going to be one week.\n",
    "        indexes = list(tdf['Date'])\n",
    "        temporarydf = inputdf[inputdf['Date'].isin(indexes)].reset_index(drop=True)\n",
    "        totaldf = pd.concat([totaldf,temporarydf],axis=0)\n",
    "        totaldf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "        df = totaldf.iloc[-15:]\n",
    "        \n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        odf = outputdf.iloc[inputdf[inputdf['Date'].isin(list(df['Date']))].index].reset_index(drop=True)\n",
    "        \n",
    "        if((end+pd.Timedelta(7,unit='D')) > pd.to_datetime(\"2017-11-19\")):\n",
    "            print \"When exiting the loop  \",end\n",
    "            break\n",
    "\n",
    "        indexes = list(inputdf[ (inputdf['Date'] > end) & (inputdf['Date'] <= (\n",
    "            end + pd.Timedelta(value=7, unit='D')))].index)\n",
    "        \n",
    "        o_indices += indexes\n",
    "        # Generating the test dataframe.\n",
    "        tdf = inputdf[(inputdf['Date'] > end) & (inputdf['Date'] <= (\n",
    "            end + pd.Timedelta(value=7, unit='D')))].reset_index(drop=True)\n",
    "\n",
    "        todf = outputdf.iloc[indexes]\n",
    "        \n",
    "        start = tdf.iloc[0][0]\n",
    "        end = tdf.iloc[tdf.shape[0]-1][0] \n",
    "        \n",
    "        if(tdf.shape[0] < 7):\n",
    "            print \"Detected a problem !!\"\n",
    "            print (\"Shape of Test Set : \",tdf.shape)\n",
    "            #break\n",
    "            \n",
    "        loopcounter += 1\n",
    "        model_init_counter+=1\n",
    "        \n",
    "    del model\n",
    "    del model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T07:30:39.736666Z",
     "start_time": "2018-04-07T06:51:04.184980Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "'''\n",
    "NN Model\n",
    "Processing data, for the model. Normalized!!!\n",
    "Features :\n",
    "(1) 4 columns pertaining to the language wise occupancies.l\n",
    "(2) Moving Average of food per occ of food items. [2,3,7,10 days of MA]\n",
    "Total Features : 8\n",
    "\n",
    "With Dropout.\n",
    "'''\n",
    "\n",
    "master_ypred = {}\n",
    "master_ytest = {}\n",
    "master_mape = {}\n",
    "history2 = {}\n",
    "indices = {}\n",
    "master_dict = {}\n",
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "outputdf = outputdf[outputdf.columns[0:14]]\n",
    "'''\n",
    "Get the indices for the weekdays and weekends in 2017.\n",
    "'''\n",
    "\n",
    "for i in range(0, 14):\n",
    "    totaldf = pd.DataFrame()\n",
    "    df = pd.DataFrame()\n",
    "    odf = pd.DataFrame()\n",
    "    tdf = pd.DataFrame()\n",
    "    todf = pd.DataFrame()\n",
    "    dates = []\n",
    "    mape = []\n",
    "    df,odf,tdf,todf,o_indices = init_dfs(df,odf,tdf,todf)\n",
    "    \n",
    "    start = tdf.iloc[0][0]\n",
    "    end = tdf.iloc[tdf.shape[0]-1][0]\n",
    "    \n",
    "    print df.shape,odf.shape,tdf.shape,todf.shape\n",
    "    o_indices = list(tdf.index)\n",
    "    totaldf = df.copy()\n",
    "    \n",
    "    loopcounter = 0\n",
    "    model_init_counter = 0\n",
    "    \n",
    "    master_dict[i] = {}\n",
    "    master_ypred[i] = []\n",
    "    master_ytest[i] = []\n",
    "    master_mape[i] = []\n",
    "    \n",
    "    history2 = {}\n",
    "    \n",
    "    while(1):\n",
    "        errcategory = []\n",
    "        print \"\\n\\n\"\n",
    "        print \"--------------------------------------------------------------\"\n",
    "        indices[loopcounter] = []\n",
    "        history2[tdf['Date'][0]] = []\n",
    "        local = []\n",
    "        counter = 0\n",
    "        '''\n",
    "        Initially, df and odf are defined.\n",
    "\n",
    "        Normalization should occur, each and every time, a new batch comes in.\n",
    "\n",
    "        Make some changes here, where df refers to the current training dataframe.\n",
    "        For each food, initially 'df' will be containing the first month of training.\n",
    "        '''\n",
    "        print \"Shape of Train df, Test df : \",df.shape,tdf.shape\n",
    "        print \"Train between : \",df.iloc[0][0],\" - \",df.iloc[-1][0]\n",
    "        start_date += [df.iloc[0][0]]\n",
    "        end_date += [df.iloc[-1][0]]\n",
    "        print \"Test between  : \",tdf.iloc[0][0],\" - \",tdf.iloc[-1][0]\n",
    "        tdf = tdf.fillna(0)\n",
    "        tdf = tdf.replace([np.inf],[0])\n",
    "        \n",
    "        df = df.fillna(0)\n",
    "        df = df.replace([np.inf],[0])\n",
    "        # Normalize both the input and the output at the same time.\n",
    "        df,tdf = normalize_df(totaldf,df, tdf)\n",
    "        #print df.head\n",
    "        test_indices = range(tdf.shape[0])\n",
    "        xTrain = ((df.values[:,[485,486,487] + range(2,6)]))\n",
    "        xTest = ((tdf.values[:, [485,486,487] + range(2,6)]))\n",
    "        lst = [6, 20, 34, 48, 426 ]\n",
    "\n",
    "        for j in lst:\n",
    "            xTrain = np.hstack((xTrain, df.values[:,i+j].reshape(-1, 1)))\n",
    "            xTest = np.hstack((xTest, tdf.values[:,i+j].reshape(-1, 1),))\n",
    "        \n",
    "        yTrain = odf.values[:,i]\n",
    "        yTest = todf.values[:,i]\n",
    "\n",
    "        print(\"\\n\")\n",
    "        if (model_init_counter==0):\n",
    "            model = nn_model_init(xTrain)\n",
    "        yPred = nn_model(xTrain, yTrain, xTest, yTest, test_indices, model,df.shape[0])\n",
    "        \n",
    "        for looper in range(yPred.shape[0]):\n",
    "            master_ypred[i] += list(yPred[looper])\n",
    "        \n",
    "        master_ytest[i] += list(yTest)\n",
    "        templist = mape_calc(yTest,yPred)\n",
    "        master_mape[i] += templist\n",
    "        dates += list(tdf['Date'])\n",
    "        \n",
    "        local.append(calMetric(yTest, yPred, i))\n",
    "        history2[tdf['Date'][0]].append(local)\n",
    "        \n",
    "        errcategory = calculate_err_category(templist)\n",
    "        # Sequential test, train split.\n",
    "        \n",
    "        # New train set is going to be one week.\n",
    "        indexes = list(tdf['Date'])\n",
    "        temporarydf = inputdf[inputdf['Date'].isin(indexes)].reset_index(drop=True)\n",
    "        totaldf = pd.concat([totaldf,temporarydf],axis=0)\n",
    "        totaldf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "        df = totaldf.iloc[-15:]\n",
    "        \n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        odf = outputdf.iloc[inputdf[inputdf['Date'].isin(list(df['Date']))].index].reset_index(drop=True)\n",
    "        \n",
    "        if((end+pd.Timedelta(7,unit='D')) > pd.to_datetime(\"2017-11-19\")):\n",
    "            print \"When exiting the loop  \",end\n",
    "            break\n",
    "\n",
    "        indexes = list(inputdf[ (inputdf['Date'] > end) & (inputdf['Date'] <= (\n",
    "            end + pd.Timedelta(value=7, unit='D')))].index)\n",
    "        \n",
    "        o_indices += indexes\n",
    "        # Generating the test dataframe.\n",
    "        tdf = inputdf[(inputdf['Date'] > end) & (inputdf['Date'] <= (\n",
    "            end + pd.Timedelta(value=7, unit='D')))].reset_index(drop=True)\n",
    "\n",
    "        todf = outputdf.iloc[indexes]\n",
    "        \n",
    "        start = tdf.iloc[0][0]\n",
    "        end = tdf.iloc[tdf.shape[0]-1][0] \n",
    "        \n",
    "        if(tdf.shape[0] < 7):\n",
    "            print \"Detected a problem !!\"\n",
    "            print (\"Shape of Test Set : \",tdf.shape)\n",
    "            #break\n",
    "            \n",
    "        loopcounter += 1\n",
    "        model_init_counter+=1\n",
    "        \n",
    "    del model\n",
    "    master_dict[i] = (history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T13:15:49.982263Z",
     "start_time": "2018-03-30T13:15:49.931744Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(start_date)):\n",
    "    print i,\" : \",start_date[i],\" - \",end_date[i],\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T15:12:03.774410Z",
     "start_time": "2020-01-13T15:12:03.223228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputdf.columns = ['food1', 'food2', 'food3', 'food4', 'food5', 'food6', 'food7',\n",
    "       'food8', 'food9', 'food10', 'food11', 'food12', 'food13', 'food14']\n",
    "\n",
    "mean = []\n",
    "median = []\n",
    "writer = pd.ExcelWriter(\"DynamicMeanMedianModel7.xlsx\")\n",
    "for food_counter in range(14):\n",
    "    mean = []\n",
    "    median = []\n",
    "    print outputdf.columns[food_counter]\n",
    "    for i in range(len(start_date)):\n",
    "        #print i,\" : \",start_date[i],\" - \",end_date[i],\"\\n\"\n",
    "        mean += [correctdf[(correctdf['Date']>=start_date[i])&(correctdf['Date']<=end_date[i])].iloc[:,80+food_counter].mean()]\n",
    "        median += [correctdf[(correctdf['Date']>=start_date[i])&(correctdf['Date']<=end_date[i])].iloc[:,80+food_counter].median()]\n",
    "    print len(mean),len(median),len(start_date),len(end_date)\n",
    "    d = pd.DataFrame(columns=[\"Training Start Date\",\"Training End Date\",\"Mean\",\"Median\"])\n",
    "    d['Training Start Date'] = start_date\n",
    "    d['Training End Date'] = end_date\n",
    "    d['Mean'] = mean\n",
    "    d['Median'] = median\n",
    "    d.to_excel(writer,sheet_name=outputdf.columns[food_counter],index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T14:55:22.745875Z",
     "start_time": "2018-03-28T14:55:22.653816Z"
    }
   },
   "outputs": [],
   "source": [
    "j=2\n",
    "for i in range(len(master_ypred[0])):\n",
    "    try:\n",
    "        if(master_ypred[0][j] == master_ypred[0][i]):\n",
    "            print i,j\n",
    "            break\n",
    "        j+=1\n",
    "    except:\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:57:14.460707Z",
     "start_time": "2018-03-29T05:57:14.321071Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(b.shape[0]):\n",
    "        if(b.iloc[i][0]!=te.iloc[i][0]):\n",
    "            print b.iloc[i][0],\" : \",te.iloc[i][0]\n",
    "            print i\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T03:47:19.926843Z",
     "start_time": "2018-03-28T03:47:19.542244Z"
    }
   },
   "outputs": [],
   "source": [
    "for key,val in master_dict.iteritems():\n",
    "    print outputdf.columns[key],\" : \",\"\\n\"\n",
    "    for key1,val1 in sorted(val.iteritems()):\n",
    "        print key1,\" : \",\"\\n\",val1,\"\\n\"\n",
    "    print \"--------------------------------\"\n",
    "    print \"\\n\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T14:56:46.905377Z",
     "start_time": "2018-03-28T14:56:46.889919Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_mape(yPred,yTest):\n",
    "    mape = []\n",
    "    for i in range(yPred.shape[0]):\n",
    "        mape.append(abs(yPred[i] - yTest[i])*1.0/yTest[i]*100)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-25T15:22:36.995942Z",
     "start_time": "2018-03-25T15:22:36.888927Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics on Randomized split.\n",
    "80-20 split.\n",
    "\n",
    "Latest Metrics NOW\n",
    "\n",
    "9PM Results\n",
    "with Spikes\n",
    "\n",
    "12 features.\n",
    "\n",
    "language occupancies of day i.\n",
    "food sales history till day i-2 (inclusive). + food sales of day 'i-1' till 6PM.\n",
    "moving average till i-2 (inclusive). + inclusive of sales of food till day 'i-1' for MA.\n",
    "\n",
    "4 language occupancies + 4 MA of food per occ of 2,3,7,10 days + 7 days of history !!\n",
    "\n",
    "'''\n",
    "for key,val in history2.items():\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"Iteration Number : \",key)\n",
    "    print (\"\\n\")\n",
    "    for j in range(len(val[0])):\n",
    "        print (outputdf.columns[j])\n",
    "        print (\"R2    : \",val[0][j][0])\n",
    "        print (\"MAE   : \",val[0][j][1])\n",
    "        print (\"RMSE  : \",val[0][j][2])\n",
    "        print (\"MAPE : \",val[0][j][3][0])\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T15:11:00.298342Z",
     "start_time": "2018-03-21T15:11:00.259388Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics on Randomized split.\n",
    "80-20 split.\n",
    "\n",
    "Latest Metrics NOW\n",
    "\n",
    "9PM Results\n",
    "without spikes\n",
    "\n",
    "12 features.\n",
    "\n",
    "language occupancies of day i.\n",
    "food sales history till day i-2 (inclusive). + food sales of day 'i-1' till 6PM.\n",
    "moving average till i-2 (inclusive). + inclusive of sales of food till day 'i-1' for MA.\n",
    "\n",
    "4 language occupancies + 4 MA of food per occ of 2,3,7,10 days + 7 days of history !!\n",
    "\n",
    "'''\n",
    "for key,val in history2.items():\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"Iteration Number : \",key)\n",
    "    print (\"\\n\")\n",
    "    for j in range(len(val[0])):\n",
    "        print (outputdf.columns[j])\n",
    "        print (\"R2    : \",val[0][j][0])\n",
    "        print (\"MAE   : \",val[0][j][1])\n",
    "        print (\"RMSE  : \",val[0][j][2])\n",
    "        print (\"MAPE : \",val[0][j][3][0])\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T06:29:34.685438Z",
     "start_time": "2018-03-20T06:29:11.974518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from xgboost import plot_importance\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed(1)\n",
    "\n",
    "'''\n",
    "Processing data, for the model. Normalized!!!\n",
    "Features :\n",
    "(1) 4 columns pertaining to the language wise occupancies.\n",
    "(2) Moving Average of food per occ of food items. [2,3,7,10 days of MA]\n",
    "Total Features : 8\n",
    "\n",
    "With Dropout.\n",
    "'''\n",
    "\n",
    "master_ypred = []\n",
    "master_ytest = []\n",
    "history2 = {}\n",
    "indices = {}\n",
    "\n",
    "tsize=int(0.8*inputdf.shape[0])\n",
    "outputdf = outputdf[outputdf.columns[0:14]]\n",
    "\n",
    "\n",
    "'''\n",
    "Get the indices for the weekdays and weekends in 2017.\n",
    "'''\n",
    "\n",
    "for loopcounter in range(1):\n",
    "    \n",
    "    indices[loopcounter] = []\n",
    "    history2[loopcounter] = []\n",
    "    local = []\n",
    "    counter = 0\n",
    "    \n",
    "    k = np.arange(inputdf.shape[0])\n",
    "    np.random.shuffle(k)\n",
    "    t = (int)(0.8*len(k))\n",
    "    train,test = k[0:t],k[t:]\n",
    "    seq_indices = {}\n",
    "    c_counter=0\n",
    "    test_indices = []\n",
    "    \n",
    "    #Convert the indices for the test set, in sequential order.\n",
    "    for c in test:\n",
    "        seq_indices[c] = c_counter\n",
    "        c_counter+=1\n",
    "    \n",
    "    df2017 = inputdf.iloc[test]\n",
    "    \n",
    "    df2017.dow = df2017.Date.dt.dayofweek\n",
    "    \n",
    "    #Weekends : (Fri,Sat,Sun)\n",
    "    weekends = (df2017[(df2017.dow==4) | (df2017.dow==5) |(df2017.dow==6)].index)\n",
    "    \n",
    "    #Weekdays : (Mon - Thurs)\n",
    "    weekdays = (df2017[(df2017.dow==0) | (df2017.dow==1) |(df2017.dow==2) | (df2017.dow==3)].index)\n",
    "    \n",
    "    weekends_indices = []\n",
    "    weekdays_indices = []\n",
    "    \n",
    "    for c in weekdays:\n",
    "        weekdays_indices.append(seq_indices[c])\n",
    "    \n",
    "    for c in weekends:\n",
    "        weekends_indices.append(seq_indices[c])\n",
    "        \n",
    "    all_days = weekends_indices + weekdays_indices\n",
    "    \n",
    "    #Get the indices of the test set. Using these indices, we'll get the corresponding dates.\n",
    "    indices[loopcounter].append(k[t:])\n",
    "    len(train),inputdf.shape,test\n",
    "    lst = [6, 20, 34, 48, 426]\n",
    "    for i in range(0,14):\n",
    "        xTrain, xTest = ((inputdf.values[:tsize,2:6])) , ((inputdf.values[tsize:,2:6]))  \n",
    "        print (xTrain.shape,xTest.shape)\n",
    "        for j in lst:\n",
    "            xTrain, xTest = np.hstack((xTrain,inputdf.values[:tsize,i+j].reshape(-1,1))),np.hstack((xTest,inputdf.values[tsize:,i+j].reshape(-1,1)))    \n",
    "\n",
    "        xTrain, xTest = np.hstack((xTrain,to_categorical((inputdf['dow']-1))[:tsize])),np.hstack((xTest,to_categorical((inputdf['dow']-1))[tsize:]))\n",
    "        yTrain, yTest = outputdf.values[:tsize,i], outputdf.values[tsize:,i]\n",
    "\n",
    "        print(xTrain.shape,xTest.shape,yTrain.shape,yTest.shape)\n",
    "        x = np.vstack((xTrain,xTest))\n",
    "        y = np.vstack((yTrain.reshape((-1,1)),yTest.reshape((-1,1))))\n",
    "\n",
    "        xTrain = pd.DataFrame(x).iloc[train].values\n",
    "        xTest  = pd.DataFrame(x).iloc[test].values\n",
    "        yTrain = pd.DataFrame(y).iloc[train].values\n",
    "        yTest  = pd.DataFrame(y).iloc[test].values\n",
    "        \n",
    "        print(xTrain.shape, yTrain.shape)\n",
    "        \n",
    "        estimator = XGBRegressor( learning_rate=0.1, n_estimators=500, max_depth=5,\n",
    "        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8, \n",
    "        nthread=4, scale_pos_weight=1,seed=27)\n",
    "        estimator.fit(xTrain,yTrain)\n",
    "        yPred = estimator.predict(xTest)\n",
    "        yTest = np.array([item for sublist in yTest for item in sublist])\n",
    "        \n",
    "        print (\"Plot\")\n",
    "        plt.figure(figsize=(15,6))\n",
    "        plt.hist(abs((yPred - yTest)*100/yTest),bins=100)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plot_importance(estimator)\n",
    "        \n",
    "        master_ypred.append(yPred)\n",
    "        master_ytest.append(yTest)\n",
    "        local.append(calMetric(yTest,yPred,i))\n",
    "        \n",
    "        del estimator\n",
    "        \n",
    "    history2[loopcounter].append(local)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T06:24:41.143022Z",
     "start_time": "2018-03-20T06:24:41.118105Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics on Randomized split.\n",
    "80-20 split.\n",
    "\n",
    "Latest Metrics NOW\n",
    "\n",
    "9PM Results\n",
    "with Spikes\n",
    "\n",
    "12 features.\n",
    "\n",
    "language occupancies of day i.\n",
    "food sales history till day i-2 (inclusive). + food sales of day 'i-1' till 6PM.\n",
    "moving average till i-2 (inclusive). + inclusive of sales of food till day 'i-1' for MA.\n",
    "\n",
    "4 language occupancies + 4 MA of food per occ of 2,3,7,10 days + 7 days of history !!\n",
    "\n",
    "'''\n",
    "for key,val in history2.items():\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"Iteration Number : \",key)\n",
    "    print (\"\\n\")\n",
    "    for j in range(len(val[0])):\n",
    "        print (outputdf.columns[j])\n",
    "        print (\"R2    : \",val[0][j][0])\n",
    "        print (\"MAE   : \",val[0][j][1])\n",
    "        print (\"RMSE  : \",val[0][j][2])\n",
    "        print (\"MAPE : \",val[0][j][3])\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T06:29:34.718930Z",
     "start_time": "2018-03-20T06:29:34.688888Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics on Randomized split.\n",
    "80-20 split.\n",
    "\n",
    "Latest Metrics NOW\n",
    "\n",
    "9PM Results\n",
    "without Spikes\n",
    "\n",
    "12 features.\n",
    "\n",
    "language occupancies of day i.\n",
    "food sales history till day i-2 (inclusive). + food sales of day 'i-1' till 6PM.\n",
    "moving average till i-2 (inclusive). + inclusive of sales of food till day 'i-1' for MA.\n",
    "\n",
    "4 language occupancies + 4 MA of food per occ of 2,3,7,10 days + 7 days of history !!\n",
    "\n",
    "'''\n",
    "for key,val in history2.items():\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"Iteration Number : \",key)\n",
    "    print (\"\\n\")\n",
    "    for j in range(len(val[0])):\n",
    "        print (outputdf.columns[j])\n",
    "        print (\"R2    : \",val[0][j][0])\n",
    "        print (\"MAE   : \",val[0][j][1])\n",
    "        print (\"RMSE  : \",val[0][j][2])\n",
    "        print (\"MAPE : \",val[0][j][3])\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T07:14:45.207846Z",
     "start_time": "2018-04-09T07:14:45.192459Z"
    }
   },
   "outputs": [],
   "source": [
    "outputdf.columns = ['food1', 'food2', 'food3', 'food4', 'food5', 'food6', 'food7',\n",
    "       'food8', 'food9', 'food10', 'food11', 'food12', 'food13', 'food14']\n",
    "'''\n",
    "The dates that you see in the excel file, will be one day less than the actual date.\n",
    "i.e. it will be '07-03-2016' instead of '08-03-2016', because we make a prediction\n",
    "today (i-1), for tomorrow.\n",
    "'''\n",
    "#d = pd.DataFrame(inputdf.iloc[test+1][\"Date\"])\n",
    "#d.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T07:14:45.539156Z",
     "start_time": "2018-04-09T07:14:45.521757Z"
    }
   },
   "outputs": [],
   "source": [
    "len (dates ) , len(master_ypred[0]) , len(master_ytest[0]) , len(master_mape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T11:10:43.865187Z",
     "start_time": "2018-04-09T11:10:43.724955Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = inputdf[(inputdf['Date']>=pd.to_datetime(\"2017-02-01\"))&(inputdf['Date']<=pd.to_datetime(\"2017-11-19\"))]['Date']\n",
    "dates.reset_index(drop=True,inplace=True)\n",
    "dates\n",
    "\n",
    "writer = pd.ExcelWriter('NewModel3NN.xlsx')\n",
    "'''\n",
    "l = pd.DataFrame(sorted(master_dict[0].keys()),columns=[\"Start Date\"])\n",
    "l[\"End Date\"] = l[\"Start Date\"]+pd.Timedelta(6,unit='D')\n",
    "l.to_excel(writer,'Date_Sheet',index=False)\n",
    "'''\n",
    "\n",
    "for j in range(14):\n",
    "    d = pd.DataFrame()\n",
    "    d['Date'] = dates\n",
    "    d['yPred'] = master_ypred[j]\n",
    "    d['yTest'] = master_ytest[j]\n",
    "    d['mape'] = master_mape[j]\n",
    "    d.to_excel(writer,sheet_name=outputdf.columns[j],index=False)\n",
    "    \n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "nbpresent": {
   "slides": {
    "9cbe1e44-cc2a-45d1-b495-e0cd85b4d526": {
     "id": "9cbe1e44-cc2a-45d1-b495-e0cd85b4d526",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
